# Related Work

## Type Checker Validation Approaches

The Python community has developed several approaches to validate type checker correctness, each with distinct strengths and limitations.

**Conformance Testing**. The Python Typing Council maintains an official conformance test suite that validates type checker behavior against PEP specifications
[@typing_conformance]. This suite tests whether implementations correctly handle prescribed typing features, such as generic types (PEP 484), protocols (PEP 544), and
TypedDict (PEP 589). However, conformance testing has significant gaps. First, coverage is incomplete—newer type checkers like ty (currently in beta) are not yet officially
included in the testing framework, though independent evaluation is possible. Second, when type checkers are evaluated against the conformance suite, scores vary
dramatically: independent testing shows ty achieving 15% conformance, pyrefly 58%, and zuban 69% [@sinon2025conformance]. Third, and most critically, conformance tests
measure adherence to specification, not robustness against real-world edge cases that fall outside the spec's explicit scope.

**Manual Test Suites**. Type checker projects maintain their own test suites, typically comprising thousands of hand-written examples. Mypy's test suite contains over 10,000
cases spanning a decade of development [@mypy_tests]. While these suites provide high-quality coverage of known patterns, they suffer from inherent scalability limitations.
As Xu et al. observe, manual test suites "can hardly keep up with rapid increase in size and complexity" of modern type systems [@xu2019hydra]. Each new PEP introduces
features that require new test cases, and the combinatorial explosion of interactions between features makes exhaustive manual coverage infeasible.

**Reactive Bug Detection**. In practice, many type checker bugs are discovered through user reports. Developers encounter edge cases during development, file GitHub issues,
and maintainers patch the bugs. This reactive approach creates substantial time lag between bug introduction and detection. Oh and Oh found that type checker bugs persist
for an average of 82 days, with 30% requiring over a month to fix [@oh2022pyter]. This delay is particularly problematic given Python's rapid evolution—Python 3.12 and 3.13
introduced major typing features (PEP 695, PEP 742) that created new opportunities for bugs before existing tools were fully validated.

**The Validation Gap**. Current approaches share a fundamental limitation: they are reactive rather than proactive. Conformance tests validate known specification
requirements; manual suites encode previously discovered patterns; user reports identify bugs only after developers encounter them. There is no systematic approach to
generating novel test cases that expose latent bugs before users encounter them in production. This gap motivates the need for automated, adversarial test generation that
can proactively explore the space of potential type checker disagreements.

## Differential Testing for Static Analyzers

