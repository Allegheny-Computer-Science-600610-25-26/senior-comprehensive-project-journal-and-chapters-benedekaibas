# Related Work

## Type Checker Validation Approaches

The Python community has developed several approaches to validate type checker correctness, each with distinct strengths and limitations.

**Conformance Testing**. The Python Typing Council maintains an official conformance test suite that validates type checker behavior against PEP specifications
[@typing_conformance]. This suite tests whether implementations correctly handle prescribed typing features, such as generic types (PEP 484), protocols (PEP 544), and
TypedDict (PEP 589). However, conformance testing has significant gaps. First, coverage is incomplete—newer type checkers like ty (currently in beta) are not yet officially
included in the testing framework, though independent evaluation is possible. Second, when type checkers are evaluated against the conformance suite, scores vary
dramatically: independent testing shows ty achieving 15% conformance, pyrefly 58%, and zuban 69% [@sinon2025conformance]. Third, and most critically, conformance tests
measure adherence to specification, not robustness against real-world edge cases that fall outside the spec's explicit scope.

**Manual Test Suites**. Type checker projects maintain their own test suites, typically comprising thousands of hand-written examples. Mypy's test suite contains over 10,000
cases spanning a decade of development [@mypy_tests]. While these suites provide high-quality coverage of known patterns, they suffer from inherent scalability limitations.
As Xu et al. observe, manual test suites "can hardly keep up with rapid increase in size and complexity" of modern type systems [@xu2019hydra]. Each new PEP introduces
features that require new test cases, and the combinatorial explosion of interactions between features makes exhaustive manual coverage infeasible.

**Reactive Bug Detection**. In practice, many type checker bugs are discovered through user reports. Developers encounter edge cases during development, file GitHub issues,
and maintainers patch the bugs. This reactive approach creates substantial time lag between bug introduction and detection. Oh and Oh found that type checker bugs persist
for an average of 82 days, with 30% requiring over a month to fix [@oh2022pyter]. This delay is particularly problematic given Python's rapid evolution—Python 3.12 and 3.13
introduced major typing features (PEP 695, PEP 742) that created new opportunities for bugs before existing tools were fully validated.

**The Validation Gap**. Current approaches share a fundamental limitation: they are reactive rather than proactive. Conformance tests validate known specification
requirements; manual suites encode previously discovered patterns; user reports identify bugs only after developers encounter them. There is no systematic approach to
generating novel test cases that expose latent bugs before users encounter them in production. This gap motivates the need for automated, adversarial test generation that
can proactively explore the space of potential type checker disagreements.

## Differential Testing for Static Analyzers

Differential testing, formalized by McKeeman in 1998, is a methodology for detecting bugs by providing identical input to multiple implementations of the same specification and identifying
discrepancies in their outputs [@mckeeman1998differential]. Unlike traditional testing, which requires an oracle—a known correct answer for each test case—differential testing exploits the principle
that consensus among independent implementations provides evidence of correctness. This approach is particularly valuable for domains like compilers and type checkers, where determining the correct
output for arbitrary inputs is difficult and constructing test oracles is expensive [@yang2011finding]. When implementations disagree, at least one must be incorrect, enabling bug detection without
manual verification.

Yang et al. demonstrated the effectiveness of differential testing for compiler validation with Csmith, a random C program generator that found over 325 previously unknown bugs in mature compilers
including GCC and LLVM [@yang2011finding]. Their approach generated syntactically valid C programs, compiled them with multiple compilers, and compared outputs. Disagreements indicated compiler bugs,
which were confirmed by examining the generated assembly or consulting language specifications. Critically, Yang et al. found that differential testing revealed bugs missed by extensive manual test
suites, suggesting that consensus-based testing exposes edge cases that structured testing overlooks.

The technique has proven effective beyond compilers. Rigger and Su applied differential testing to database management systems, finding bugs in SQLite, MySQL, and PostgreSQL by generating semantically
equivalent SQL queries and comparing results [@rigger2020finding]. Kapus and Cadar used differential testing to validate symbolic execution engines, identifying disagreements between KLEE, S2E, and
other tools [@kapus2019differential]. These successes demonstrate that differential testing generalizes to any domain with multiple independent implementations of a common specification.

Python type checkers present an ideal target for differential testing. Multiple implementations (mypy, pyrefly, zuban, ty) claim to implement the same typing specification, yet they differ in maturity,
implementation language, and optimization strategies. When these tools disagree on whether code is type-correct, the disagreement signals either a specification ambiguity or an implementation bug.
However, a critical limitation emerges: unlike compilers where runtime execution provides ground truth, type checker disagreements alone cannot determine which tool is correct. This motivates the need
for an independent oracle—a challenge we address through runtime testing.

## Automated Test Generation

Automated test generation techniques have proven effective at discovering bugs in software systems, offering an alternative to manual test construction that can explore program behaviors
systematically. These techniques broadly fall into three categories: mutation-based fuzzing, grammar-based fuzzing, and constraint-based approaches.

Mutation-based fuzzing generates test inputs by randomly modifying seed inputs, observing program behavior on the mutated inputs. Coverage-guided greybox fuzzing, exemplified by tools like American
Fuzzy Lop (AFL), enhances this approach by using code coverage feedback to guide mutation toward unexplored program paths [@bohme2016coverage]. Böhme et al. model this process as a Markov chain,
demonstrating that assigning energy inversely proportional to path frequency improves efficiency by gravitating toward low-frequency paths. Their approach found numerous bugs in widely-tested systems,
showing that random mutation with coverage feedback can effectively explore complex program behaviors. However, for programs with highly structured inputs—such as programming languages—purely random
mutation often generates syntactically invalid inputs that are rejected before reaching interesting code paths.

Grammar-based fuzzing addresses this limitation by leveraging language grammars to generate syntactically valid inputs. Holler et al. introduced LangFuzz, which parses code fragments from existing
test suites and mutates them according to language grammar rules [@holler2012fuzzing]. Applied to JavaScript engines, LangFuzz discovered 105 severe vulnerabilities in Mozilla's interpreter within
three months, demonstrating that grammar-aware generation can efficiently target language processors. Similarly, Yang et al. developed Csmith, a random C program generator that found over 325 bugs in
mature compilers including GCC and LLVM [@yang2011finding]. These successes highlight a key insight: domain-specific knowledge about input structure dramatically improves fuzzing effectiveness for
language processors.

Constraint-based approaches, particularly symbolic execution, offer an alternative by systematically exploring program paths through constraint solving. Cadar et al. developed KLEE, a symbolic
execution engine that achieved over 90% line coverage on GNU COREUTILS utilities and discovered bugs that had persisted for over 15 years [@cadar2008klee]. KLEE symbolically executes programs,
maintaining path constraints for each execution path and using constraint solvers to generate inputs that reach specific program locations. While powerful for achieving high coverage in systems code,
symbolic execution faces scalability challenges: path explosion limits analysis to relatively small programs or short execution depths, and constraint solving becomes prohibitively expensive for
complex path conditions.

For type checker testing, existing automated test generation techniques face specific challenges. Random fuzzing, which generates inputs through random mutations, struggles with dynamic
languages—Lukasczyk et al. found that lack of type information and Python's dynamic nature pose fundamental obstacles for automated test generation [@lukasczyk2020automated]. Grammar-based approaches
can generate syntactically valid code but struggle to produce semantically meaningful programs. Padhye et al. note that while syntax generators can produce structurally valid inputs, ensuring semantic
validity—inputs that satisfy domain-specific constraints—remains difficult [@padhye2019semantic]. For type checkers, this manifests as code that parses correctly but fails to exercise complex type
system features like generic constraints, protocol inheritance, or type narrowing. Symbolic execution is poorly suited to type checkers, which perform static analysis rather than concrete execution,
making symbolic execution's path-based reasoning less applicable. Moreover, none of these approaches leverage the rich signal available in existing bug reports—a source of information about precisely
which language features and type constructs tend to trigger checker errors. This gap motivates our bug-seeded mutation approach, which combines domain knowledge from real bugs with LLM-based code
generation to produce test cases that are both syntactically valid and semantically targeted at problematic type checker behaviors.

