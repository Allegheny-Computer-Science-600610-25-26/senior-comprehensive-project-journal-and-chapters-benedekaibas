# Related Work

## Type Checker Validation Approaches

The Python community has developed several approaches to validate type checker correctness, each with distinct strengths and limitations.

**Conformance Testing**. The Python Typing Council maintains an official conformance test suite that validates type checker behavior against PEP specifications
[@typing_conformance]. This suite tests whether implementations correctly handle prescribed typing features, such as generic types (PEP 484), protocols (PEP 544), and
TypedDict (PEP 589). However, conformance testing has significant gaps. First, coverage is incomplete—newer type checkers like ty (currently in beta) are not yet officially
included in the testing framework, though independent evaluation is possible. Second, when type checkers are evaluated against the conformance suite, scores vary
dramatically: independent testing shows ty achieving 15% conformance, pyrefly 58%, and zuban 69% [@sinon2025conformance]. Third, and most critically, conformance tests
measure adherence to specification, not robustness against real-world edge cases that fall outside the spec's explicit scope.

**Manual Test Suites**. Type checker projects maintain their own test suites, typically comprising thousands of hand-written examples. Mypy's test suite contains over 10,000
cases spanning a decade of development [@mypy_tests]. While these suites provide high-quality coverage of known patterns, they suffer from inherent scalability limitations.
As Xu et al. observe, manual test suites "can hardly keep up with rapid increase in size and complexity" of modern type systems [@xu2019hydra]. Each new PEP introduces
features that require new test cases, and the combinatorial explosion of interactions between features makes exhaustive manual coverage infeasible.

**Reactive Bug Detection**. In practice, many type checker bugs are discovered through user reports. Developers encounter edge cases during development, file GitHub issues,
and maintainers patch the bugs. This reactive approach creates substantial time lag between bug introduction and detection. Oh and Oh found that type checker bugs persist
for an average of 82 days, with 30% requiring over a month to fix [@oh2022pyter]. This delay is particularly problematic given Python's rapid evolution—Python 3.12 and 3.13
introduced major typing features (PEP 695, PEP 742) that created new opportunities for bugs before existing tools were fully validated.

**The Validation Gap**. Current approaches share a fundamental limitation: they are reactive rather than proactive. Conformance tests validate known specification
requirements; manual suites encode previously discovered patterns; user reports identify bugs only after developers encounter them. There is no systematic approach to
generating novel test cases that expose latent bugs before users encounter them in production. This gap motivates the need for automated, adversarial test generation that
can proactively explore the space of potential type checker disagreements.

## Differential Testing for Static Analyzers

Differential testing, formalized by McKeeman in 1998, is a methodology for detecting bugs by providing identical input to multiple implementations of the same specification and identifying
discrepancies in their outputs [@mckeeman1998differential]. Unlike traditional testing, which requires an oracle—a known correct answer for each test case—differential testing exploits the principle
that consensus among independent implementations provides evidence of correctness. This approach is particularly valuable for domains like compilers and type checkers, where determining the correct
output for arbitrary inputs is difficult and constructing test oracles is expensive [@yang2011finding]. When implementations disagree, at least one must be incorrect, enabling bug detection without
manual verification.

Yang et al. demonstrated the effectiveness of differential testing for compiler validation with Csmith, a random C program generator that found over 325 previously unknown bugs in mature compilers
including GCC and LLVM [@yang2011finding]. Their approach generated syntactically valid C programs, compiled them with multiple compilers, and compared outputs. Disagreements indicated compiler bugs,
which were confirmed by examining the generated assembly or consulting language specifications. Critically, Yang et al. found that differential testing revealed bugs missed by extensive manual test
suites, suggesting that consensus-based testing exposes edge cases that structured testing overlooks.

The technique has proven effective beyond compilers. Rigger and Su applied differential testing to database management systems, finding bugs in SQLite, MySQL, and PostgreSQL by generating semantically
equivalent SQL queries and comparing results [@rigger2020finding]. Kapus and Cadar used differential testing to validate symbolic execution engines, identifying disagreements between KLEE, S2E, and
other tools [@kapus2019differential]. These successes demonstrate that differential testing generalizes to any domain with multiple independent implementations of a common specification.

Python type checkers present an ideal target for differential testing. Multiple implementations (mypy, pyrefly, zuban, ty) claim to implement the same typing specification, yet they differ in maturity,
implementation language, and optimization strategies. When these tools disagree on whether code is type-correct, the disagreement signals either a specification ambiguity or an implementation bug.
However, a critical limitation emerges: unlike compilers where runtime execution provides ground truth, type checker disagreements alone cannot determine which tool is correct. This motivates the need
for an independent oracle—a challenge we address through runtime testing.

