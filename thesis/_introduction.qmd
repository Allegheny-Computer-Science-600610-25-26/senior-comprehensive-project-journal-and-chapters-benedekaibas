---
title: "An Empirical Study on Static Analyzer Toolsets to Reduce False Positives, False Negatives in Python Type Checkers"
date: "2025-10-11"
---

# Introduction

By 2025, Python has established itself as the dominant programming language, a trend corroborated by major industry indices including the
IEEE Spectrum and TIOBE [1, 2]. Its ecosystem is now foundational to diverse sectors, ranging from web and enterprise development to embedded
systems and artificial intelligence. However, Python’s flexibility comes at a cost: a susceptibility to runtime failures. Research indicates
that type-related errors are among the most prevalent runtime exceptions in Python development [3]. These errors, which occur when an operation
is performed on an incompatible data type, account for a significant portion of defects in open-source Python projects [4].

## Motivation

The adoption of static type checkers in Python has grown significantly over the past years. Major technology companies and open-source projects
have integrated type checking into their development workflows to catch bugs early in the development cycle [5, 6]. Tools like mypy, pyright,
pyre, and pytype have become essential components in many Python projects' continuous integration pipelines [7, 8, 9]. As the reliance on these
tools increases, so does the importance of their correctness and reliability.

Recently, the open-source community has introduced several high-performance type checkers, including pyrefly, zuban, and ty [10, 11, 12].
While these tools claim to offer superior execution speeds compared to industry standards, their correctness and reliability remain unverified.
Crucially, there is currently no comprehensive framework benchmarking these emerging tools against established static analyzers. Furthermore,
there is a distinct lack of research evaluating their behavior on newly generated code examples specifically designed to identify false
positive reports.

When developers trust a type checker to validate their code, they expect the tool to accurately identify genuine type errors while not flagging
correct code as erroneous. A type checker that produces too many false positives can lead to developer frustration and eventual abandonment of
the tool, a phenomenon known as warning fatigue [13]. Developers may start ignoring warnings or adding unnecessary type annotations to silence
the checker, which defeats the purpose of static analysis. On the other hand, a type checker that produces false negatives provides a false
sense of security. Developers might believe their code is type-safe when it actually contains bugs that could manifest at runtime and
potentially cause system failures in production environments [3].

The Python type system has evolved considerably since the introduction of type hints in PEP 484 [14]. New features such as protocols, type
guards, variadic generics, and the `@member` decorator for enums have been added to the language [15, 16, 17]. Each new feature introduces
additional complexity that type checkers must handle correctly. As demonstrated in the examples provided in the introduction, even mature type
checkers like mypy can struggle with these newer constructs. This ongoing evolution of the type system creates a continuous need for testing
and validation of type checker implementations.

Furthermore, the dynamic nature of Python presents a fundamental theoretical challenge for static analysis: the trade-off between soundness
and completeness. In type theory, a sound system guarantees that no ill-typed program is accepted (no false negatives), while a complete
system ensures that no well-typed program is rejected (no false positives). Due to Python's highly dynamic features—such as runtime class
modification, eval, and the extensive use of the Any type—achieving perfect soundness is computationally undecidable [@siek2015refined].
Consequently, practical type checkers must rely on the principles of Gradual Typing, which intentionally relaxes soundness guarantees to
maintain usability [@vitousek2014design]. This inherent compromise creates a fragile boundary where inconsistencies and "soundness holes"
frequently emerge, further necessitating the need for rigorous automated testing.

Currently, the process of finding bugs in type checkers relies heavily on user-reported issues and manual test case creation by the development
teams. This approach has limitations because it depends on users encountering edge cases during their regular development work and taking the
time to report them. Many bugs in type checkers may go undetected for extended periods simply because no one has written code that triggers
them. Our automated approach to re-generate test cases based on closed GitHub issues from type checkers that can expose false positives and
false negatives would complement the existing testing strategies and help identify issues before users encounter them. Furthermore, the
diversity of type checkers available for Python means that the same code can produce different results across different tools. Understanding
these differences and identifying cases where type checkers disagree can help both developers choose the right tool for their needs and type
checker maintainers improve consistency across the ecosystem.

This research is motivated by the need to systematically improve the quality of Python type checkers through automated testing.
By developing a tool that can generate test cases capable of triggering false reports, we aim to reduce the burden on type checker development
teams and ultimately improve the reliability of static type analysis in Python.

### The Importance of Using Automated Test Generation

According to Oh and Oh [@oh2022pyter], the timeline for resolving type errors varies significantly: while 31.4% of reported errors are patched
within a single day, 29.4% require more than a month to resolve. Furthermore, bugs included in the PyTER benchmark [@oh2022pyter] persisted for
an average of 82 days, with approximately 30% taking longer than a month to fix.

These statistics indicate that the manual identification and reporting of type-related issues in open-source projects is a slow,
resource-intensive process. This latency is particularly problematic given the rapid evolution of the Python programming language, which
frequently introduces new syntax and features [@pep602]. To maintain effective type error detection, type checkers must be developed and
updated concurrently with these language changes. Reliance on manual testing and user reports following a new Python release is inherently
reactive and time-consuming, preventing the ecosystem from adopting a proactive stance toward type safety.

### Current State of the Art

Current research into Python static typing is diverse, encompassing empirical studies on type checkers usage [@khan2022empirical], comparative
analyses of tool performance [@oh2024towards], and the application of fuzzing techniques for bug detection [@xifaras2025enumerative]. While
these distinct lines of inquiry exist, there is a significant gap in the literature: currently, no study integrates automated fuzzing with
empirical analysis to systematically evaluate the reliability of the most well known type checkers.

Historically, the dynamic nature of Python has presented inherent challenges for static analysis, often limiting its reach across the broader
spectrum of Python users. However, a noticeable shift is occurring within the industry. Major technology firms and large-scale open-source
projects are increasingly integrating static typing [@distefano2019scaling] to ensure maintainability and prevent regressions in massive
codebases. This growing demand has catalyzed the development of numerous new type checkers, yet the rigor of their validation remains an open
question.

#### Addressing The Gap

Despite advancements in static analysis, there is currently no research or automated tooling that systematically generates high-level code
examples derived from closed issue reports from type checkers. Specifically, current researches have not targeted the automated ingestion of
previously submitted issues to refine—or mutate—them into new test cases designed to trigger false reports.

Pytifex addresses this void by automating the generation of complex, high-level test cases derived from the history of resolved bugs. The tool
operates by mining closed GitHub issues to extract code snippets that previously triggered false reports (false positives & false negatives).
While these specific issues have been patched, Pytifex utilizes them as high-quality "seed" examples. It systematically refines—or mutates—
these seeds to create new, adversarial test cases that retain the structural complexity of the original bug, but introduce novel variations
capable of triggering false reports in the latest stable versions of established type checkers.

