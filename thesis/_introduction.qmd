---
title: "An Empirical Study on Static Analyzer Toolsets to Reduce False Positives, False Negatives in Python Type Checkers"
date: "2025-10-11"
---

# Introduction

By 2025, Python has established itself as the dominant programming language, a trend corroborated by major industry indices including the
IEEE Spectrum and TIOBE [1, 2]. Its ecosystem is now foundational to diverse sectors, ranging from web and enterprise development to embedded
systems and artificial intelligence. However, Python’s flexibility comes at a cost: a susceptibility to runtime failures. Research indicates
that type-related errors are among the most prevalent runtime exceptions in Python development [3]. These errors, which occur when an operation
is performed on an incompatible data type, account for a significant portion of defects in open-source Python projects [4].

## Motivation

The adoption of static type checkers in Python has grown significantly over the past years. Major technology companies and open-source projects
have integrated type checking into their development workflows to catch bugs early in the development cycle [5, 6]. Tools like mypy, pyright,
pyre, and pytype have become essential components in many Python projects' continuous integration pipelines [7, 8, 9]. As the reliance on these
tools increases, so does the importance of their correctness and reliability.

Recently, the open-source community has introduced several high-performance type checkers, including pyrefly, zuban, and ty [10, 11, 12].
While these tools claim to offer superior execution speeds compared to industry standards, their correctness and reliability remain unverified.
Crucially, there is currently no comprehensive framework benchmarking these emerging tools against established static analyzers. Furthermore,
there is a distinct lack of research evaluating their behavior on newly generated code examples specifically designed to identify false
positive reports.

When developers trust a type checker to validate their code, they expect the tool to accurately identify genuine type errors while not flagging
correct code as erroneous. A type checker that produces too many false positives can lead to developer frustration and eventual abandonment of
the tool, a phenomenon known as warning fatigue [13]. Developers may start ignoring warnings or adding unnecessary type annotations to silence
the checker, which defeats the purpose of static analysis. On the other hand, a type checker that produces false negatives provides a false
sense of security. Developers might believe their code is type-safe when it actually contains bugs that could manifest at runtime and
potentially cause system failures in production environments [3].

The Python type system has evolved considerably since the introduction of type hints in PEP 484 [14]. New features such as protocols, type
guards, variadic generics, and the @member decorator for enums have been added to the language [15, 16, 17]. Each new feature introduces
additional complexity that type checkers must handle correctly. As demonstrated in the examples provided in the introduction, even mature type
checkers like mypy can struggle with these newer constructs. This ongoing evolution of the type system creates a continuous need for testing
and validation of type checker implementations.

Currently, the process of finding bugs in type checkers relies heavily on user-reported issues and manual test case creation by the development
teams. This approach has limitations because it depends on users encountering edge cases during their regular development work and taking the
time to report them. Many bugs in type checkers may go undetected for extended periods simply because no one has written code that triggers
them. Our automated approach to re-generate test cases based on closed GitHub issues from type checkers that can expose false positives and
false negatives would complement the existing testing strategies and help identify issues before users encounter them. Furthermore, the
diversity of type checkers available for Python means that the same code can produce different results across different tools. Understanding
these differences and identifying cases where type checkers disagree can help both developers choose the right tool for their needs and type
checker maintainers improve consistency across the ecosystem.

This research is motivated by the need to systematically improve the quality of Python type checkers through automated testing.
By developing a tool that can generate test cases capable of triggering false reports, we aim to reduce the burden on type checker development
teams and ultimately improve the reliability of static type analysis in Python.

## The Importance of Using Automated Test Generation

According to Oh and Oh [@oh2022pyter], the timeline for resolving type errors varies significantly: while 31.4% of reported errors are patched
within a single day, 29.4% require more than a month to resolve. Furthermore, bugs included in the PyTER benchmark [@oh2022pyter] persisted for
an average of 82 days, with approximately 30% taking longer than a month to fix.

These statistics indicate that the manual identification and reporting of type-related issues in open-source projects is a slow,
resource-intensive process. This latency is particularly problematic given the rapid evolution of the Python programming language, which
frequently introduces new syntax and features [@pep602]. To maintain effective type error detection, type checkers must be developed and
updated concurrently with these language changes. Reliance on manual testing and user reports following a new Python release is inherently
reactive and time-consuming, preventing the ecosystem from adopting a proactive stance toward type safety.

## Current State of the Art

Current research into Python static typing is diverse, encompassing empirical studies on type checkers usage [@khan2022empirical], comparative
analyses of tool performance [@oh2024towards], and the application of fuzzing techniques for bug detection [@xifaras2025enumerative]. While
these distinct lines of inquiry exist, there is a significant gap in the literature: currently, no study integrates automated fuzzing with
empirical analysis to systematically evaluate the reliability of the most well known type checkers.

Historically, the dynamic nature of Python has presented inherent challenges for static analysis, often limiting its reach across the broader
spectrum of Python users. However, a noticeable shift is occurring within the industry. Major technology firms and large-scale open-source
projects are increasingly integrating static typing [@distefano2019scaling] to ensure maintainability and prevent regressions in massive
codebases. This growing demand has catalyzed the development of numerous new type checkers, yet the rigor of their validation remains an open
question.

### Addressing The Gap

Despite advancements in static analysis, there is currently no research or automated tooling that systematically generates high-level code
examples derived from closed issue reports from type checkers. Specifically, current researches have not targeted the automated ingestion of
previously submitted issues to refine—or mutate—them into new test cases designed to trigger false reports.

Pytifex addresses this void by automating the generation of complex, high-level test cases derived from the history of resolved bugs. The tool
operates by mining closed GitHub issues to extract code snippets that previously triggered false reports (false positives & false negatives).
While these specific issues have been patched, Pytifex utilizes them as high-quality "seed" examples. It systematically refines—or mutates—
these seeds to create new, adversarial test cases that retain the structural complexity of the original bug but introduce novel variations capable
of triggering false reports in the latest stable versions of established type checkers.

## Contributions

This research builds upon several researches and extending existing researches while also using already existing agent framework.






















# Introduction

According to StackOverflow, IEEE, and other surveys Python has became the most popular programming language by 2025. We can see that Python programming language
is used in various fields, such as: web development, enterprise, embedded domains, machine learning, and artificial intelligence. However, Python suffers from runtime
errors. The leading runtime errors are type related errors. Type error happens when an operation is performed on a value of an unsupported type.

```{python}
def foo(number: int) -> int:
  """Return the input as a squared number."""
  return "The squared number is: ", number**2
```

In this case even though the programmer wanted to return a nice way of representing the squared input number type error occured. Since in the function declaration it is 
stated that the return type of this function has to be an integer the following implementation is wrong, because it returns a tuple that contains string and integer.

To catch these type related errors in large code bases type checkers can provide a significant amount of help. Type checkers can detect type related errors in Python programs
statically which means developers do not have to run the programs in order to see, if there is an error in their program. However, static analyzer are not perfect and they can
produce false positive and false negative reports.

By definition false positive means that the static analyer reported an error that is not an actual bug in the code.

```{python}
from enum import Enum, member
from typing import reveal_type

class E(Enum):
  @member
  def A() -> int:
    return 1

assert E.A.value() == 1
reveal_type(E.A)
reveal_type(E.A.value)
```

```{text}
fp01.py:6: error: Method must have at least one argument. Did you forget the "self" argument?
```

This is a typical case where the type checker (mypy with version: 1.18.2) provided a false positive report. This issue got later fixed in the mypy type checker.
False positives cause time overhead in terms of overviewing a code that is actually correct, so by reducing false positive reports in type checkers will lead to
the point where programmers have to spend less time dealing with false positive reports and more time on fixing actual bugs.

When a program has a bug in it, but the static analyzer misses catching it that case called false negative report. In this case the type checker(s) missed a bug that
was indeed in the program's code. This is a serious issue because, if this bug does not get detected by static analyzers and neither test cases and goes to production
that can cause bugs in the given system, service, etc. where the code is used.

```{python}
from typing import Protocol

class CanHex(Protocol):
  def hex(self, /) -> str: ...

int_does_not_have_hex: CanHex = 959
bool_does_not_have_hex: CanHex = False
```

```{text}
Success: no issues found in 1 source file
```

This example were taken from a real example where the mypy type checker with the 1.13 version missed detecting the bug. This issue got later resolved as well as the false positive case!

Through these examples it is clear that in software engineering static analyzers play a crucial role, but -just as any other tools- they have their cons as well.

My goal with this research is to provide feedback for developer teams working on these type checkers on where their type checkers should be improved and also provide an automated tool that 
generates new, uncovered examples that are able to trigger type checkers to provide false reports.

## Motivation

The adoption of static type checkers in Python has grown significantly over the past years. Major technology companies and open-source
projects have integrated type checking into their development workflows to catch bugs early in the development cycle. Tools like mypy,
pyright, pyre, and pytype have become essential components in many Python projects' continuous integration pipelines.As the reliance on these 
tools increases, so does the importance of their correctness and reliability.

When developers trust a type checker to validate their code, they expect the tool to accurately identify genuine type errors while not 
flagging correct code as erroneous. A type checker that produces too many false positives can lead to developer frustration and eventual
abandonment of the tool. Developers may start ignoring warnings or adding unnecessary type annotations to silence the checker,
which defeats the purpose of static analysis. On the other hand, a type checker that produces false negatives provides a false sense of
security. Developers might believe their code is type-safe when it actually contains bugs that could manifest at runtime and potentially 
cause system failures in production environments.

The Python type system has evolved considerably since the introduction of type hints in PEP 484. New features such as protocols, type guards,
variadic generics, and the @member decorator for enums have been added to the language. Each new feature introduces additional complexity that
type checkers must handle correctly. As demonstrated in the examples provided in the introduction, even mature type checkers like mypy can 
struggle with these newer constructs. This ongoing evolution of the type system creates a continuous need for testing and validation of type
checker implementations.

Currently, the process of finding bugs in type checkers relies heavily on user-reported issues and manual test case creation by the development
teams. This approach has limitations because it depends on users encountering edge cases during their regular development work and taking the
time to report them. Many bugs in type checkers may go undetected for extended periods simply because no one has written code that triggers them.
An automated approach to generating test cases that can expose false positives and false negatives would complement the existing testing
strategies and help identify issues before users encounter them. Furthermore, the diversity of type checkers available for Python means that
the same code can produce different results across different tools. A program that passes mypy's checks might fail pyright's checks, or vice
versa. Understanding these differences and identifying cases where type checkers disagree can help both developers choose the right tool for 
their needs and type checker maintainers improve consistency across the ecosystem.

This research is motivated by the need to systematically improve the quality of Python type checkers through automated testing. By developing a tool that can generate test cases capable of triggering false reports,
we aim to reduce the burden on type checker development teams and ultimately improve the reliability of static type analysis in Python. The practical outcome of this work will be a more robust type checking
ecosystem that developers can trust to accurately validate their code.

## 


## Research Goals

Goal of my research is to include statistical data in this paper on how famous type checkers behaved on the automatically generated code examples that were created by the automated tool called pytifex that supports the
purpose of my research. By doing this empirical stdu
