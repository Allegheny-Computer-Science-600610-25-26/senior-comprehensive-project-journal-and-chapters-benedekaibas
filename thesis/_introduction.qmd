---
title: "Chapter One of My Senior Thesis"
date: "2025-10-11"
---

# Introduction

According to StackOverflow, IEEE, and other surveys Python has became the most popular programming language by 2025. We can see that Python programming language
is used in various fields, such as: web development, enterprise, embedded domains, machine learning, and artificial intelligence. However, Python suffers from runtime
errors. The leading runtime errors are type related errors. Type error happens when an operation is performed on a value of an unsupported type.

```{ython}
def foo(number: int) -> int:
  """Return the input as a squared number."""
  return "The squared number is: ", number**2
```

In this case even though the programmer wanted to return a nice way of representing the squared input number type error occured. Since in the function declaration it is 
stated that the return type of this function has to be an integer the following implementation is wrong, because it returns a tuple that contains string and integer.

To catch these type related errors in large code bases type checkers can provide a significant amount of help. Type checkers can detect type related errors in Python programs
statically which means developers do not have to run the programs in order to see, if there is an error in their program. However, static analyzer are not perfect and they can
produce false positive and false negative reports.

By definition false positive means that the static analyer reported an error that is not an actual bug in the code.

```{python}
from enum import Enum, member
from typing import reveal_type

class E(Enum):
  @member
  def A() -> int:
    return 1

assert E.A.value() == 1
reveal_type(E.A)
reveal_type(E.A.value)
```

```{text}
fp01.py:6: error: Method must have at least one argument. Did you forget the "self" argument?
```

This is a typical case where the type checker (mypy with version: 1.18.2) provided a false positive report. This issue got later fixed in the mypy type checker.
False positives cause time overhead in terms of overviewing a code that is actually correct, so by reducing false positive reports in type checkers will lead to
the point where programmers have to spend less time dealing with false positive reports and more time on fixing actual bugs.

When a program has a bug in it, but the static analyzer misses catching it that case called false negative report. In this case the type checker(s) missed a bug that
was indeed in the program's code. This is a serious issue because, if this bug does not get detected by static analyzers and neither test cases and goes to production
that can cause bugs in the given system, service, etc. where the code is used.

```{python}
from typing import Protocol

class CanHex(Protocol):
  def hex(self, /) -> str: ...

int_does_not_have_hex: CanHex = 959
bool_does_not_have_hex: CanHex = False
```

```{text}
Success: no issues found in 1 source file
```

This example were taken from a real example where the mypy type checker with the 1.13 version missed detecting the bug. This issue got later resolved as well as the false positive case!

Through these examples it is clear that in software engineering static analyzers play a crucial role, but -just as any other tools- they have their cons as well.

My goal with this research is to provide feedback for developer teams working on these type checkers on where their type checkers should be improved and also provide an automated tool that 
generates new, uncovered examples that are able to trigger type checkers to provide false reports.

## Motivation

The adoption of static type checkers in Python has grown significantly over the past years. Major technology companies and open-source projects have integrated type checking into their development 
workflows to catch bugs early in the development cycle. Tools like mypy, pyright, pyre, and pytype have become essential components in many Python projects' continuous integration pipelines.
As the reliance on these tools increases, so does the importance of their correctness and reliability.

When developers trust a type checker to validate their code, they expect the tool to accurately identify genuine type errors while not flagging correct code as erroneous. A type checker that produces
too many false positives can lead to developer frustration and eventual abandonment of the tool. Developers may start ignoring warnings or adding unnecessary type annotations to silence the checker,
which defeats the purpose of static analysis. On the other hand, a type checker that produces false negatives provides a false sense of security. Developers might believe their code is type-safe when
it actually contains bugs that could manifest at runtime and potentially cause system failures in production environments.

The Python type system has evolved considerably since the introduction of type hints in PEP 484. New features such as protocols, type guards, variadic generics, and the @member decorator for enums have
been added to the language. Each new feature introduces additional complexity that type checkers must handle correctly. As demonstrated in the examples provided in the introduction, even mature type checkers
like mypy can struggle with these newer constructs. This ongoing evolution of the type system creates a continuous need for testing and validation of type checker implementations.

Currently, the process of finding bugs in type checkers relies heavily on user-reported issues and manual test case creation by the development teams. This approach has limitations because it depends on users encountering edge cases during their regular development work and taking the time to report them. Many bugs in type checkers may go undetected for extended periods simply because no one has written code that triggers them. An automated approach to generating test cases that can expose false positives and false negatives would complement the existing testing strategies and help identify issues before users encounter them.
Furthermore, the diversity of type checkers available for Python means that the same code can produce different results across different tools. A program that passes mypy's checks might fail pyright's checks, or vice versa. Understanding these differences and identifying cases where type checkers disagree can help both developers choose the right tool for their needs and type checker maintainers improve consistency across the ecosystem.
This research is motivated by the need to systematically improve the quality of Python type checkers through automated testing. By developing a tool that can generate test cases capable of triggering false reports, we aim to reduce the burden on type checker development teams and ultimately improve the reliability of static type analysis in Python. The practical outcome of this work will be a more robust type checking ecosystem that developers can trust to accurately validate their code.

## Research Goals

My research paper aims to provide a tool that can automatically generate code examples that would force the type checkers to report false negatives, false positives, and valid reports.
Then it would be able to determine which type checkers were correct. Based on this information my research paper will evaluate the different type checkers, suggest what changes would
be needed for type checkers, and provide code examples that are introducing new cases where type checkers' reports are false.



