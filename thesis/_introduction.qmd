---
title: "An Empirical Study on Static Analyzer Toolsets to Reduce False Positives, False Negatives in Python Type Checkers"
date: "2025-10-11"
---

# Introduction

By 2025, Python has established itself as the dominant programming language, a trend corroborated by major industry indices including the
IEEE Spectrum and TIOBE [1, 2]. Its ecosystem is now foundational to diverse sectors, ranging from web and enterprise development to embedded
systems and artificial intelligence. However, Python’s flexibility comes at a cost: a susceptibility to runtime failures. Research indicates
that type-related errors are among the most prevalent runtime exceptions in Python development [3]. These errors, which occur when an operation
is performed on an incompatible data type, account for a significant portion of defects in open-source Python projects [4].

## Motivation

The adoption of static type checkers in Python has grown significantly over the past years. Major technology companies and open-source projects
have integrated type checking into their development workflows to catch bugs early in the development cycle [5, 6]. Tools like mypy, pyright,
pyre, and pytype have become essential components in many Python projects' continuous integration pipelines [7, 8, 9]. As the reliance on these
tools increases, so does the importance of their correctness and reliability.

Recently, the open-source community has introduced several high-performance type checkers, including pyrefly, zuban, and ty [10, 11, 12].
While these tools claim to offer superior execution speeds compared to industry standards, their correctness and reliability remain unverified.
Crucially, there is currently no comprehensive framework benchmarking these emerging tools against established static analyzers. Furthermore,
there is a distinct lack of research evaluating their behavior on newly generated code examples specifically designed to identify false
positive reports.

When developers trust a type checker to validate their code, they expect the tool to accurately identify genuine type errors while not flagging
correct code as erroneous. A type checker that produces too many false positives can lead to developer frustration and eventual abandonment of
the tool, a phenomenon known as warning fatigue [13]. Developers may start ignoring warnings or adding unnecessary type annotations to silence
the checker, which defeats the purpose of static analysis. On the other hand, a type checker that produces false negatives provides a false
sense of security. Developers might believe their code is type-safe when it actually contains bugs that could manifest at runtime and
potentially cause system failures in production environments [3].

The Python type system has evolved considerably since the introduction of type hints in PEP 484 [14]. New features such as protocols, type
guards, variadic generics, and the @member decorator for enums have been added to the language [15, 16, 17]. Each new feature introduces
additional complexity that type checkers must handle correctly. As demonstrated in the examples provided in the introduction, even mature type
checkers like mypy can struggle with these newer constructs. This ongoing evolution of the type system creates a continuous need for testing
and validation of type checker implementations.

Furthermore, the dynamic nature of Python presents a fundamental theoretical challenge for static analysis: the trade-off between soundness
and completeness. In type theory, a sound system guarantees that no ill-typed program is accepted (no false negatives), while a complete
system ensures that no well-typed program is rejected (no false positives). Due to Python's highly dynamic features—such as runtime class
modification, eval, and the extensive use of the Any type—achieving perfect soundness is computationally undecidable [@siek2015refined].
Consequently, practical type checkers must rely on the principles of Gradual Typing, which intentionally relaxes soundness guarantees to
maintain usability [@vitousek2014design]. This inherent compromise creates a fragile boundary where inconsistencies and "soundness holes"
frequently emerge, further necessitating the need for rigorous automated testing.

Currently, the process of finding bugs in type checkers relies heavily on user-reported issues and manual test case creation by the development
teams. This approach has limitations because it depends on users encountering edge cases during their regular development work and taking the
time to report them. Many bugs in type checkers may go undetected for extended periods simply because no one has written code that triggers
them. Our automated approach to re-generate test cases based on closed GitHub issues from type checkers that can expose false positives and
false negatives would complement the existing testing strategies and help identify issues before users encounter them. Furthermore, the
diversity of type checkers available for Python means that the same code can produce different results across different tools. Understanding
these differences and identifying cases where type checkers disagree can help both developers choose the right tool for their needs and type
checker maintainers improve consistency across the ecosystem.

This research is motivated by the need to systematically improve the quality of Python type checkers through automated testing.
By developing a tool that can generate test cases capable of triggering false reports, we aim to reduce the burden on type checker development
teams and ultimately improve the reliability of static type analysis in Python.

### The Importance of Using Automated Test Generation

According to Oh and Oh [@oh2022pyter], the timeline for resolving type errors varies significantly: while 31.4% of reported errors are patched
within a single day, 29.4% require more than a month to resolve. Furthermore, bugs included in the PyTER benchmark [@oh2022pyter] persisted for
an average of 82 days, with approximately 30% taking longer than a month to fix.

These statistics indicate that the manual identification and reporting of type-related issues in open-source projects is a slow,
resource-intensive process. This latency is particularly problematic given the rapid evolution of the Python programming language, which
frequently introduces new syntax and features [@pep602]. To maintain effective type error detection, type checkers must be developed and
updated concurrently with these language changes. Reliance on manual testing and user reports following a new Python release is inherently
reactive and time-consuming, preventing the ecosystem from adopting a proactive stance toward type safety.

### Current State of the Art

Current research into Python static typing is diverse, encompassing empirical studies on type checkers usage [@khan2022empirical], comparative
analyses of tool performance [@oh2024towards], and the application of fuzzing techniques for bug detection [@xifaras2025enumerative]. While
these distinct lines of inquiry exist, there is a significant gap in the literature: currently, no study integrates automated fuzzing with
empirical analysis to systematically evaluate the reliability of the most well known type checkers.

Historically, the dynamic nature of Python has presented inherent challenges for static analysis, often limiting its reach across the broader
spectrum of Python users. However, a noticeable shift is occurring within the industry. Major technology firms and large-scale open-source
projects are increasingly integrating static typing [@distefano2019scaling] to ensure maintainability and prevent regressions in massive
codebases. This growing demand has catalyzed the development of numerous new type checkers, yet the rigor of their validation remains an open
question.

#### Addressing The Gap

Despite advancements in static analysis, there is currently no research or automated tooling that systematically generates high-level code
examples derived from closed issue reports from type checkers. Specifically, current researches have not targeted the automated ingestion of
previously submitted issues to refine—or mutate—them into new test cases designed to trigger false reports.

Pytifex addresses this void by automating the generation of complex, high-level test cases derived from the history of resolved bugs. The tool
operates by mining closed GitHub issues to extract code snippets that previously triggered false reports (false positives & false negatives).
While these specific issues have been patched, Pytifex utilizes them as high-quality "seed" examples. It systematically refines—or mutates—
these seeds to create new, adversarial test cases that retain the structural complexity of the original bug, but introduce novel variations
capable of triggering false reports in the latest stable versions of established type checkers.

## Related Work

### Empirical Analysis of Python Type Errors

The prevalence and impact of type-related errors in Python have been the subject of several recent empirical studies, highlighting the
significant role they place on software maintenance. Research by Khan et al. [@khan2022empirical] provides a comprehensive analysis of
defects in Python projects, revealing the potential impact of static analysis on code quality. Their study found that approximately 15% of
corrective defects (and 11% of all defects) in Python projects could likely have been avoided by using a static type checker. This statistic
underscores the inherent risks of Python's dynamic typing discipline, where preventable type inconsistencies often remain dormant until
execution.

Furthermore, the process of resolving these errors is far from trivial. Oh and Oh [@oh2022pyter] investigated the lifecycle of type defects
and observed that manual detection and repair are resource-intensive tasks. Their analysis of the "PyTER" benchmark showed that while roughly
one-third of type errors are patched within a day, nearly 30% persist for over a month, with the average resolution time extending to 82 days.
This variance suggests that while some type errors are superficial, a significant portion involve complex logic that developers struggle to
diagnose and fix promptly.

These findings collectively confirm the high cost of type errors in the software development lifecycle. The data indicates that reliance on
manual testing and reactive bug fixing is insufficient to keep pace with the volume of type defects. Consequently, there is a critical need
for reliable, automated static analysis tools capable of identifying these issues early in the development process, before they turn into
costly runtime failures or long-standing technical debt.

### The Landscape of Python Static Analysis

The classification of programming languages into static and dynamic typing paradigms fundamentally dictates how correctness is verified. In
statically typed languages (e.g., C++, C, Java), variable types are resolved at compile-time, allowing the compiler to enforce strict constraints
and guarantee that certain classes of errors are absent before execution. Conversely, Python employs a dynamic typing discipline where type
checking is deferred until runtime. This approach offers significant advantages in developer velocity, enabling rapid prototyping, concise
syntax, and high flexibility through features like runtime reflection and metaprogramming.

### Challenges of Static Analysis in Dynamic Languages

Despite these ergonomic benefits, dynamic typing introduces severe challenges for static analysis. The primary disadvantage is the lack of
explicit type information, which forces static analyzers to rely on complex inference algorithms that are often heuristic rather than provable.

Specifically, Python's dynamic features makes static analysis harder for the Python programming language:

- **Undecidability**: Features such as `eval()`, `getattr()`, and dynamic class creation allow code to modify its own structure at runtime.
  Accurately predicting the types in such programs is statically undecidable, meaning a tool cannot guarantee correctness without executing
  the code.

- **Soundness Gaps**: To remain usable, Python type checkers must adopt gradual typing [@siek2015refined]. As defined by the official Python
  Typing Specification, unannotated parameters and variables are often implicitly treated as the Any type, effectively disabling type checking
  for those regions to prevent false positives [@python_typing_spec]. This creates a trade-off where the checker is neither fully sound
  (it misses true errors in unannotated code) nor fully complete (it may flag valid dynamic patterns as erroneous).

- **Fragile Inference**: In the absence of annotations, inference engines often struggle with heterogeneous collections and variable recycling.
  For instance, initializing a list with mixed types (e.g., `data = [1, "two"]`) forces the checker to choose a common base type. Tools like
  Mypy typically infer `List[object]`, which technically captures both elements but causes false positives when the developer attempts valid
  operations (like string manipulation) on the items. Similarly, reusing a variable name for different types (`x = 10; x = "text"`) is valid
  Python, but frequently confuses analyzers that "lock" the inferred type to the first assignment, flagging the second as an error.

### Established and Emerging Tools

To mitigate these risks, the Python ecosystem has adopted a range of static type checkers. These tools vary significantly in their architecture,
performance, and adherence to soundness guarantees.

#### Mypy

Mypy As the reference implementation for Python static typing (PEP 484), Mypy is the most widely adopted tool in the industry. It introduced
the concept of gradual typing to Python, allowing teams to migrate codebases incrementally. While Mypy is robust and features a vast ecosystem
of plugin supports (e.g., for Django or NumPy), its performance can be a bottleneck in large monolithic repositories.

#### Pyrefly

Pyrefly Developed by Meta, Pyrefly is a high-performance, Rust-based type checker designed to handle the scale of massive "monorepos." Its
primary goal is to provide instant feedback to developers, even in projects with millions of lines of code. By focusing its analysis only on
the specific parts of the codebase affected by a change—rather than re-checking the entire project—Pyrefly avoids the performance bottlenecks
common in older tools like Mypy. This makes it exceptionally well-suited for interactive use within IDEs, where speed and responsiveness are
critical.

#### Zuban

Zuban focuses on the intersection of static analysis and developer tooling. Written in Rust for performance, Zuban aims to provide
Mypy-compatible diagnostics while significantly reducing execution time. Its primary differentiation lies in its "resilient" parsing
capability, designed to provide useful type feedback even in broken or incomplete syntax trees, making it particularly effective for IDE
integrations where code is constantly in a state of changes.

#### Ty

Ty is a newcomer from the open-source community that positions itself as the high-speed alternative for modern Python development. Its main
selling point is raw performance; benchmarks often show it running 10 to 20 times faster than established tools . The goal with Ty is to make
type checking feel as instant as linting or formatting, integrating tightly into that workflow. However, because it prioritizes speed and
low-latency feedback, it is still an open question whether it can handle messy, legacy typing behaviors as strictly and reliably as the other
well tested Python type checkers.

### Automated Fuzzing and Test Generation

The validation of compilers and static analyzers has historically relied on fuzzing—the automated generation of random test cases to uncover
edge cases that manual testing often misses. In the domain of statically typed languages, this technique has reached a high degree of maturity.
The seminal work by Yang et al. on Csmith [@yang2011finding] established the efficacy of differential testing, where a random program is fed to
multiple compilers (e.g., GCC and LLVM) to detect discrepancies. While Csmith successfully identified hundreds of bugs in C compilers, its
techniques rely on the rigid, static constraints of the C language, which do not translate directly to dynamic environments.


